{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS475 Natural Language Model - Programming Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to submit**\n",
    "* Fill out <mark>TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
    "* Submit one file: hw1_{student_ID}.ipynb to KLMS\n",
    "    e.g.hw1_20243150.ipynb\n",
    "* **Late submission policy**: After the submission deadline, you will\n",
    "immediately lose 10% of the score, another 10% after 24 hours later, and so on.\n",
    "Submission after 72 hours (3 days) will not be counted.\n",
    "\n",
    "**Note**\n",
    "* Make a copy of this .ipynb file. Do not directly edit this file.\n",
    "* You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
    "* Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
    "* TA will look into the implemented functions, their validity and give corresponding score to each <mark>TODO</mark> problem.\n",
    "* Ask questions through KLMS so that you can share information with other students.\n",
    "* TA in charge: Yeahoon Kim (yeahoon.kim@kaist.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this programming assignment, you will**\n",
    "* Learn how the backporpagation works.\n",
    "* Learn how the pytorch module works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivation of a simple function with one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is derivation?\n",
    "\n",
    "Derivation is a mathematical process used to find the rate of change of a function with respect to its input variables. It is an essential tool in calculus and is widely used in various fields of science and engineering.\n",
    "\n",
    "In programming, we can approximate the derivative of a function by using numerical methods. One common method is called the **\"finite difference approximation.\"**\n",
    "\n",
    "The finite difference approximation calculates the derivative by estimating the slope of the function at a given point. It does this by taking the difference between the function values at two nearby points and dividing it by the difference in their corresponding input values. This can be expressed as an equation as follows:\n",
    "\n",
    "$\\frac{f(x+h)-f(x)}{h}$\n",
    "\n",
    "You can check the detail in [here](https://en.wikipedia.org/wiki/Finite_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Simple example of the derivation\n",
    "The following is the basic example of the derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## TODO: Implement the gradient function ##################\n",
    "# The gradient function should take a function f and a point x and return the gradient of f at x.\n",
    "# The gradient should be approximated using finite differences with a small step size h.\n",
    "# The function f is assumed to be scalar-valued.\n",
    "def gradient(f, x: float, h=1e-5) -> float:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return 0\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function f(x)\n",
    "def f(x: float) -> float:\n",
    "    return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "# Find the gradient of f(x) at x = 2\n",
    "h = 0.00001\n",
    "x = 2\n",
    "df = gradient(f, x, h)\n",
    "\n",
    "print(f\"Gradient of function f at the point {x}: {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Implement value object for the gradient flow\n",
    "The `Value` object is a class that represents a scalar value and its gradient. It is used in the context of gradient descent and backpropagation algorithms. The `Value` object has the following attributes and methods:\n",
    "\n",
    "Attributes:\n",
    "- `data`: The scalar value stored in the object.\n",
    "- `grad`: The gradient of the scalar value with respect to some variable.\n",
    "- `_backward`: A function that defines the backward pass of the backpropagation algorithm.\n",
    "- `_prev`: A set of `Value` objects that are the inputs to the operation that produced this object.\n",
    "- `_op`: A string that represents the operation that produced this object.\n",
    "\n",
    "Methods:\n",
    "- `__add__(self, other)`: Overloads the `+` operator to perform addition between two `Value` objects or a `Value` object and a scalar.\n",
    "- `__mul__(self, other)`: Overloads the `*` operator to perform multiplication between two `Value` objects or a `Value` object and a scalar.\n",
    "- `__pow__(self, other)`: Overloads the `**` operator to perform exponentiation between a `Value` object and an integer or float.\n",
    "- `relu(self)`: Applies the rectified linear unit (ReLU) activation function to the `Value` object.\n",
    "- `backward(self)`: Performs the backward pass of the backpropagation algorithm to compute the gradients of the `Value` object and its inputs.\n",
    "\n",
    "The `Value` object is used in the implementation of neural networks and other machine learning algorithms to compute gradients and update the model parameters during training. It allows for automatic differentiation and efficient computation of gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>TODO</mark> Implement the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    ############# TODO: implement more activation functions ############\n",
    "    # You can implement more activations such as sigmoid, tanh, etc.\n",
    "    # Make sure to implement their gradients as well.\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        # You can refer to the following link to implement the sigmoid function\n",
    "        # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "        pass\n",
    "    \n",
    "    def tanh(self):\n",
    "        # You can refer to the following link to implement the tanh function\n",
    "        # https://en.wikipedia.org/wiki/Hyperbolic_function\n",
    "        pass\n",
    "    \n",
    "    def SiLU(self):\n",
    "        # You can refer to the following link to implement the SiLU function\n",
    "        # https://arxiv.org/abs/1702.03118\n",
    "        pass\n",
    "    \n",
    "    def GELU(self):\n",
    "        # You can refer to the following link to implement the GELU function\n",
    "        # https://arxiv.org/abs/1606.08415\n",
    "        pass\n",
    "    \n",
    "    def ELU(self):\n",
    "        # You can refer to the following link to implement the ELU function\n",
    "        # https://arxiv.org/abs/1511.07289\n",
    "        pass\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "c = a + b\n",
    "d = a * b + b**3\n",
    "c += c + 1\n",
    "c += 1 + c + (-a)\n",
    "d += d * 2 + (b + a).relu()\n",
    "d += 3 * d + (b - a).relu()\n",
    "e = c - d\n",
    "f = e**2\n",
    "g = f / 2.0\n",
    "g += 10.0 / f\n",
    "\n",
    "print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n",
    "g.backward()\n",
    "print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n",
    "print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use Neural Networks in MNIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
